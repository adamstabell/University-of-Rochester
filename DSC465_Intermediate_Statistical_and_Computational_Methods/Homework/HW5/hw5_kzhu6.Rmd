---
title: "DSC 465, Homework 5"
author: "Kefu Zhu"
date: "04/24/2019"
output:
  html_document:
    highlight: tango
    toc: yes
  pdf_document:
    toc: yes
  word_document:
    toc: yes
---

```{r message=FALSE, warning=FALSE}
# Load library
library(MASS)
library(knitr)
library(splines)
library(glmnet)
library(latex2exp)
library(wooldridge)
library(meifly)
# Seed number for this assignment
my_seed = 007 # :o
```

# Question 1

## (a)

$g(x)$ is a piecewise regression with three knots $\xi_1, \xi_2$ and $\xi_3$. In our case, it is a combination of constant prediction and cubic spline regression. In order to have a continuous $g(x)$, `property (ii)`, we need to have three constraints, one for each knot

<center>
$\begin{cases}a_0 = a_1 + b_1 \xi_1 + c_1 \xi_1^2 + d_1 \xi_1^3 \\ \\ a_1 + b_1 \xi_2 + c_1 \xi_2^2 + d_1 \xi_2^3 = a_2 + b_2 \xi_2 + c_2 \xi_2^2 + d_2 \xi_2^3 \\ \\ a_2 + b_2 \xi_3 + c_2 \xi_3^2 + d_2 \xi_3^3 = a_3\end{cases}$
</center>
<br>

Similarly, to fulfill the continuity of first derivative at each knot, `property (iii)`, we need the following three constraints

<center>
$\begin{cases}0 = b_1 + 2c_1 \xi_1 + 3d_1 \xi_1^2 \\ \\ b_1 + 2c_1 \xi_2 + 3d_1 \xi_2^2 = b_2 + 2c_2 \xi_2 + 3d_2 \xi_2^2 \\ \\ b_2 + 2c_2 \xi_3 + 3d_2 \xi_3^2 = 0\end{cases}$
</center>
<br>

**Answer**: In total, we need $6$ linear constraints

## (b)

Since we have $10$ parameters to estimate and we already have $6$ constaints, only $10-6=4$ free parameters are required to define $g(x)$

# Question 2
```{r}
# Transformation
log.price = log(Cars93$Price)
log.mpg = log(Cars93$MPG.city)
```

##(a)
```{r}
# Fitting models
fit_1 = lm(log.mpg ~ log.price)
fit_2 = lm(log.mpg ~ log.price + I(log.price^2))
fit_3 = lm(log.mpg ~ I(log.price^(-1)))
fit_4 = lm(log.mpg ~ I(log.price^(-2)))
# Linear spline for model 5
fit_5 = lm(log.mpg ~ bs(log.price,knots = c(2.5),degree=1))
# Linear spline for model 6
fit_6 = lm(log.mpg ~ bs(log.price,knots = c(2.5,3.0),degree=1))
```

```{r}
# Extract number of degree of freedoms from lm object
get_df = function(fit){
  return(length(fit$coefficients))
}
# Comput SSE for lm object
get_SSE = function(fit){
  return(sum(fit$residuals^2))
}
# Compute AIC for lm object based on SSE
AIC_SSE = function(fit){
  sse = get_SSE(fit)
  k = get_df(fit)
  n = length(fit$residuals)
  aic = n*log(sse/n) + 2*k
  return(aic)
}
# Compute BIC for lm object based on SSE
BIC_SSE = function(fit){
  sse = get_SSE(fit)
  k = get_df(fit)
  n = length(fit$residuals)
  bic = n*log(sse/n) + log(n)*k
  return(bic)
}
```

```{r}
# Construct lists for SSE, AIC, BIC and df
df_list = c(get_df(fit_1),get_df(fit_2),get_df(fit_3),
            get_df(fit_4),get_df(fit_5),get_df(fit_6))
SSE_list = c(get_SSE(fit_1),get_SSE(fit_2),get_SSE(fit_3),
             get_SSE(fit_4),get_SSE(fit_5),get_SSE(fit_6))
AIC_SSE_list = c(AIC_SSE(fit_1),AIC_SSE(fit_2),AIC_SSE(fit_3),
                 AIC_SSE(fit_4),AIC_SSE(fit_5),AIC_SSE(fit_6))
BIC_SSE_list = c(BIC_SSE(fit_1),BIC_SSE(fit_2),BIC_SSE(fit_3),
                 BIC_SSE(fit_4),BIC_SSE(fit_5),BIC_SSE(fit_6))
# Put all lists in a dataframe
model_eval_sse = data.frame(
  'model' = c('M1','M2','M3','M4','M5','M6'),
  'df' = df_list,
  'SSE' = SSE_list,
  'AIC' = AIC_SSE_list,
  'BIC' = BIC_SSE_list
  )
```

```{r, fig.align='center'}
# # Print the dataframe as table
# knitr::kable(model_eval_sse)
```

## (b)
```{r}
# Compute AIC for lm object based on log likelihood
AIC_LL = function(fit){
  ll = logLik(fit)
  k = get_df(fit)
  n = length(fit$residuals)
  aic = -2*ll + 2*k
  return(aic)
}
# Compute BIC for lm object based on log likelihood
BIC_LL = function(fit){
  ll = logLik(fit)
  k = get_df(fit)
  n = length(fit$residuals)
  bic = -2*ll + log(n)*k
  return(bic)
}
```

```{r}
# Construct lists for AIC, BIC based on log likelihood
AIC_LL_list = c(AIC_LL(fit_1),AIC_LL(fit_2),AIC_LL(fit_3),
                AIC_LL(fit_4),AIC_LL(fit_5),AIC_LL(fit_6))
BIC_LL_list = c(BIC_LL(fit_1),BIC_LL(fit_2),BIC_LL(fit_3),
                BIC_LL(fit_4),BIC_LL(fit_5),BIC_LL(fit_6))
# Put all lists in a dataframe
model_eval_ll = data.frame(
  'model' = c('M1','M2','M3','M4','M5','M6'),
  'df' = df_list,
  'SSE' = SSE_list,
  'AIC' = AIC_LL_list,
  'BIC' = BIC_LL_list
  )
```

## (c)
```{r}
# Construct lists for AIC, BIC based on log likelihood
AIC_list = c(AIC(fit_1),AIC(fit_2),AIC(fit_3),
                AIC(fit_4),AIC(fit_5),AIC(fit_6))
BIC_list = c(BIC(fit_1),BIC(fit_2),BIC(fit_3),
                BIC(fit_4),BIC(fit_5),BIC(fit_6))
# Put all lists in a dataframe
model_eval = data.frame(
  'model' = c('M1','M2','M3','M4','M5','M6'),
  'df' = df_list,
  'SSE' = SSE_list,
  'AIC' = AIC_list,
  'BIC' = BIC_list
  )
```

## (d)
```{r}
# Standardize AIC and BIC scores for each method
AIC_SSE_standardized = AIC_SSE_list - min(AIC_SSE_list)
AIC_ll_standardized = AIC_LL_list - min(AIC_LL_list)
AIC_standardized = AIC_list - min(AIC_list)

BIC_SSE_standardized = BIC_SSE_list - min(BIC_SSE_list)
BIC_ll_standardized = BIC_LL_list - min(BIC_LL_list)
BIC_standardized = BIC_list - min(BIC_list)
```

```{r}
# Compared standardized AIC scores from different calculation methods
data.frame(
  'model' = c('M1','M2','M3','M4','M5','M6'),
  'AIC_SSE_standardized' = AIC_SSE_standardized, 
  'AIC_ll_standardized' = AIC_ll_standardized,
  'AIC_standardized' = AIC_standardized
)
```

```{r}
# Compared standardized BIC scores from different calculation methods
data.frame(
  'BIC_SSE_standardized' = BIC_SSE_standardized, 
  'BIC_ll_standardized' = BIC_ll_standardized,
  'BIC_standardized' = BIC_standardized
)
```

**Answer**: 

Based on the comparison results above, we can conclude that the three model selection procedures are equivalent.

## (e)
```{r, fig.align='center', fig.height=9, fig.width=7}
# Set up graph grid
par(mfrow=c(3,2))
# x-grid for prediction
xgrid = seq(min(log.price),max(log.price),0.05)
# Plot for M1
plot(log.price, log.mpg)
lines(xgrid, predict(fit_1, data.frame(log.price = xgrid)))
title(paste('M1 AIC*=', 
            round(AIC_standardized[1],2),
            ' BIC*=',
            round(BIC_standardized[1],2)))
# Plot for M2
plot(log.price, log.mpg)
lines(xgrid, predict(fit_2, data.frame(log.price = xgrid)))
title(paste('M2 AIC*=', 
            round(AIC_standardized[2],2),
            ' BIC*=',
            round(BIC_standardized[2],2)))
# Plot for M3
plot(log.price, log.mpg)
lines(xgrid, predict(fit_3, data.frame(log.price = xgrid)))
title(paste('M3 AIC*=', 
            round(AIC_standardized[3],2),
            ' BIC*=',
            round(BIC_standardized[3],2)))
# Plot for M4
plot(log.price, log.mpg)
lines(xgrid, predict(fit_4, data.frame(log.price = xgrid)))
title(paste('M4 AIC*=', 
            round(AIC_standardized[4],2),
            ' BIC*=',
            round(BIC_standardized[4],2)))
# Plot for M5
plot(log.price, log.mpg)
lines(xgrid, predict(fit_5, newdata = list(log.price = xgrid)))
title(paste('M5 AIC*=', 
            round(AIC_standardized[5],2),
            ' BIC*=',
            round(BIC_standardized[5],2)))
# Plot for M6
plot(log.price, log.mpg)
lines(xgrid, predict(fit_6, newdata = list(log.price = xgrid)))
title(paste('M6 AIC*=', 
            round(AIC_standardized[6],2),
            ' BIC*=',
            round(BIC_standardized[6],2)))
```

## (f)

**Answer**:

- **Identify the models with the optimal AIC and BIC scores**:

  `Model 4` has the best AIC and BIC scores in general
  
- **Suppose this model selection application is to be based on the AIC score. Examining the plots, is there a distinct reason why the models with the second or third lowest AIC might be used in place of the optimal AIC model?**

  Because we can clearly see the tail of `Model 2` is dragged away by the data point with largest value of `log.price`, so compared to `Model 2`, the optimal AIC model, `Model 4` and `Model 6` are more robust. 
  
  `Model 4` is still the best among these three because it has descent model performance in terms of both AIC and BIC score, as well as low model complexity.

# Question 3

## (a)
```{r}
### Select variables by column index
vari = c(1,4,5,7,9,15,20,21)
### Use log-transform for response variable
y = log10(lawsch85$salary)
### Feature matrix
x = as.matrix(lawsch85[,vari])
### Create data frame and remove missing values
yx = data.frame(y,x)
yx = na.omit(yx)
### Extract separate response and feature matrix
x = as.matrix(yx[,-1])
```

### Fit second-order polynomial regression for each predictor
```{r}
# Fit all 8 models
fit.rank = lm(y ~ poly(rank,2), data = yx)
fit.LSAT = lm(y ~ poly(LSAT,2), data = yx)
fit.GPA = lm(y ~ poly(GPA,2), data = yx)
fit.faculty = lm(y ~ poly(faculty,2), data = yx)
fit.clsize = lm(y ~ poly(clsize,2), data = yx)
fit.studfac = lm(y ~ poly(studfac,2), data = yx)
fit.llibvol = lm(y ~ poly(llibvol,2), data = yx)
fit.lcost = lm(y ~ poly(lcost,2), data = yx)
```

```{r}
# Extract r-square value from all models and put them in one vector
r.squared_list = c(summary(fit.rank)$r.squared,
                   summary(fit.LSAT)$r.squared,
                   summary(fit.GPA)$r.squared,
                   summary(fit.faculty)$r.squared,
                   summary(fit.clsize)$r.squared,
                   summary(fit.studfac)$r.squared,
                   summary(fit.llibvol)$r.squared,
                   summary(fit.lcost)$r.squared)
# Add names
names(r.squared_list) = names(yx)[-1]
# Show it
r.squared_list
```

**Answer**

`rank` appears to be most informative of salary in the context of second-order polynomial regression model

### Fit all 8 predictors in one model
```{r, fig.align='center'}
# Fit the model
fit = lm(y ~ ., data = yx)
# Plot residuals vs. fitted values
plot(fit$fitted.values, fit$residuals)
```

**Answer**:

The shape of curvature on the graph indicates an non-linear relationship between response variable and the predictors. So it is not suitable for us to use a linear model.

## (b)
```{r}
# Construct the new feature matrix
X_prime = cbind(poly(yx$rank,2),poly(yx$LSAT,2),
                poly(yx$GPA,2),poly(yx$faculty,2),
                poly(yx$clsize,2),poly(yx$studfac,2),
                poly(yx$llibvol,2),poly(yx$lcost,2))
colnames(X_prime) = c('rank','rank^2','LSAT','LSAT^2','GPA','GPA^2',
                   'faculty','faculty^2','clsize','clsize^2',
                   'studfac','studfac^2','llibvol','llibvol^2',
                   'lcost','lcost^2')
# Compute the variance within each column
apply(X_prime, 2, var) # 2 means compute the variance by column
```

**Note**: The column variances are basically same. 

But they are not exactly the same. If we execute the following code. We will see $5$ different values of variance. But the difference is negligible. (We actually cannot even see the difference in the default decimal setting)

```{r}
# Check if all variances are equal
unique(apply(X_prime, 2, var))
```

```{r, fig.align='center'}
# Create new dataframe
yx_prime = as.data.frame(cbind(yx$y, X_prime))
# Add column name for response variable
colnames(yx_prime)[1] = 'y'
# Fit the model
fit_poly = lm(y ~ ., data = yx_prime)
# Plot residuals vs. fitted values
plot(fit_poly$fitted.values, fit_poly$residuals)
```

**Answer**:

Compared to the plot in Part (a), this plot does not have any noticeable special pattern, which is a good indication of not having any non-linear relationships between response variable and predictors.

## (c)
```{r}
# Set seed for Cross Validation
set.seed(my_seed)
# Fit Lasso with default 10-fold Cross-Validation
fit_lasso = cv.glmnet(x = X_prime, y = yx$y, alpha = 1)
fit_lasso_lambda1se = glmnet(
  x = X_prime,
  y = yx$y,
  alpha = 1,
  lambda = fit_lasso$lambda.1se
  )
```

```{r}
# Variables included in the 1se solution
names(fit_lasso_lambda1se$beta[,1])[fit_lasso_lambda1se$beta[,1] != 0]
```


**Answer**:

Only `rank`, `rank^2`, `LSAT`, `GPA` and `llibvol` are included in the solution with $\lambda =$  `r fit_lasso$lambda.1se` (`lambda.1e`)

## (d)
```{r}
# Set seed for Cross Validation
set.seed(my_seed)
# Fit Ridge with default 10-fold Cross-Validation
fit_ridge = cv.glmnet(x = X_prime, y = yx$y, alpha = 0)
# Fit the Ridge with lambda = lambda.1se
fit_ridge_lambda1se = glmnet(
  x = X_prime,
  y = yx$y,
  alpha = 0,
  lambda = fit_ridge$lambda.1se
  )
```

```{r}
# Rank the features for ridge model
rank(abs(coef(fit_ridge_lambda1se)[,1]))
```

```{r}
# Rank the features for lasso model
rank(abs(coef(fit_lasso_lambda1se)[,1]))
```

## (e)

### Computation Time
```{r}
# Forward selection based on AIC
forward_AIC = stepAIC(fit_poly, direction = 'forward', trace = FALSE)
system.time(stepAIC(fit_poly, direction = 'forward', trace = FALSE))
```

```{r}
# Backward selection based on AIC
backward_AIC = stepAIC(fit_poly, direction = 'backward', trace = FALSE)
system.time(stepAIC(fit_poly, direction = 'backward', trace = FALSE))
```

```{r message=FALSE, warning=FALSE, results = 'hide'}
# Perform the all-subset model selection
all_subset = fitall(yx_prime$y, yx_prime[,-1])
all_subset_runtime = system.time(fitall(yx_prime$y, yx_prime[,-1]))
```

```{r}
# Show the runtime of all-subset model selection
all_subset_runtime
```


```{r}
# Extract AIC for all models in all-subset selection
all_subset_AIC = t(sapply(all_subset, extractAIC))
# Extract the best AIC model from all-subset selection
best_all_subset = all_subset[order(all_subset_AIC[,2])[1]][[1]]
# Remove 2^16 (65535) fitted models
rm(all_subset)
# Free memory
gc()
```

```{r}
# library(leaps)
# # Perform all_subset model selection
# all_subset = regsubsets(y~., data = yx_prime, nbest = 1, nvmax = NULL)
# # Obtain the summary result from all-subset
# all_subset_summary = summary(all_subset)
# # Find the index of best model (index = # of predictors)
# best_index = which(all_subset_summary$bic == min(all_subset_summary$bic))
# # Show predictors from the best model through all-subset selection
# all.subset_features = names(all_subset_summary$which[1,])[all_subset_summary$which[best_index,]]
```

### Features Comparison

```{r}
# Show the predictors from the 1se LASSO
lasso_features = names(fit_lasso_lambda1se$beta[,1])[fit_lasso_lambda1se$beta[,1]!=0]
lasso_features
```


```{r}
# Show the predictors from the best model through forward selection
forward_features = names(forward_AIC$coefficients)
forward_features
```

```{r}
# Show the predictors from the best model through backward selection
backward_features = names(backward_AIC$coefficients)
backward_features
```

```{r}
# Show the predictors from the best model through all-subset selection
all.subset_features = names(best_all_subset$coefficients)
all.subset_features
```

```{r}
# Predictors that are included in all fitted models
Reduce(intersect,list(lasso_features,forward_features,backward_features,all.subset_features))
```

**Answer**:

- **Computation Time**:

  **Forward selection** has the smallest computation time, while **all-subset selection** takes the longest to compute. 

- **Features**: 

  Only `rank` and `LSAT` are included in all fitted models. The result is consistent with the analysis of $R^2$ from Part (a). `rank` has the highest $R^2$ value and `LSAT` has the second highest $R^2$ value.

## (f)

```{r, fig.align='center'}
# Scatterplot of salary vs. rank
plot(
  yx$rank,
  10 ^ (yx$y),
  pch = 20,
  cex = 0.8,
  xlab = 'rank',
  ylab = 'salary',
  main = 'Salary vs. Rank'
  )
# Add LASSO 1se model
points(yx$rank, 10^(predict(fit_lasso_lambda1se, newx = X_prime)), col = 'coral', pch = 1)
# Add all-subsets model
points(yx$rank, 10^(best_all_subset$fitted.values), col = 'blue2', pch = 4)
# Add legend
legend('topright',
        legend = c('LASSO 1se','all-subsets model'),
        col = c('coral','blue2'),
        pch = c(1,4),
        cex = 0.8)
```

**Answer**:

As we can see on the graph above, the fitted values of `LASSO 1se` is fairly consistent and do not seem to be affected too much by some noisy data points.

However, the model from all-subsets selection do seem to accomodate the trainig data points a lot, even for some noisy data points.

So in general, compared to the model from all-subsets selection, The `LASSO 1se` model seems to less overfit the data, which is an evidence of "shrinkage effect"

For sanity check, we look at the magnitude of coefficients for both models (down below). The magnitudes of coefficients in `LASSO 1se` model is fairly small, except one dominant predictor, `rank`.

```{r}
# Coefficients of LASSO 1se
fit_lasso_lambda1se$beta[,1][fit_lasso_lambda1se$beta[,1]!=0]
```

```{r}
# Coefficients of model from all-subsets selection
best_all_subset$coefficients
```


# Question 4

The likelihood for gaussian multiple regression model can be written as

$Likelihood = \prod \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{1}{2\sigma^2} (y_i -\beta x_i)^2} = (\frac{1}{2\pi\sigma^2})^{\frac{n}{2}} e^{-\frac{1}{2} (Y-\beta X)^T(\sigma^2)^{-1}(Y-\beta X)}$


Then the log-likelihood of the model, $LL$, is

<center>
$ln(LL) = -\frac{n}{2} ln(2\pi\sigma^2) -\frac{1}{2} (Y-\beta X)^T(\sigma^2)^{-1}(Y-\beta X)$

$-2 \cdot ln(LL) = nln(2\pi) + nln(\sigma^2) + \sum_{i=1}^n \frac{(y_i - \hat{y_i})^2}{\sigma^2}$
</center>

$\because$ The MLE of $\sigma^2$ is $SSE/n$, and $SSE = \sum_{i=1}^n (y_i - \hat{y_i})^2$

$\therefore -2 \cdot ln(LL) = nln(2\pi) + nln(\frac{SSE}{n}) + \sum_{i=1}^n \frac{(y_i - \hat{y_i})^2}{SSE/n} = nln(2\pi) + nln(\frac{SSE}{n}) + n$

Then the difference between $-2ln(LL)$ and $nln(\frac{SSE}{n})$ is

<center>
$nln(2\pi) + n$
</center>

which is a constant depends only on $n$
